{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to the data\n",
    "- Read admissions.csv into a Dataframe named admissions.\n",
    "- Use the Matplotlib method scatter to generate a scatter plot with the:\n",
    "    - gpa column on the x-axis.\n",
    "    - admit column on the y-axis.\n",
    "- Use plt.show() to display the scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "admissions = pd.read_csv(\"admissions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x26745132320>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(admissions[\"gpa\"],admissions[\"admit\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>admit</th>\n",
       "      <th>gpa</th>\n",
       "      <th>gre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3.177277</td>\n",
       "      <td>594.102992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>3.412655</td>\n",
       "      <td>631.528607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2.728097</td>\n",
       "      <td>553.714399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3.093559</td>\n",
       "      <td>551.089985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3.141923</td>\n",
       "      <td>537.184894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>3.599108</td>\n",
       "      <td>442.763567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>3.238972</td>\n",
       "      <td>667.472189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>3.420177</td>\n",
       "      <td>561.713905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>3.562482</td>\n",
       "      <td>590.340371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>3.910495</td>\n",
       "      <td>463.470183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>3.264341</td>\n",
       "      <td>636.453166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>3.308623</td>\n",
       "      <td>604.405558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>3.018922</td>\n",
       "      <td>567.714830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>3.151834</td>\n",
       "      <td>503.307298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>3.298133</td>\n",
       "      <td>628.178205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>2.652201</td>\n",
       "      <td>644.300261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>3.115603</td>\n",
       "      <td>533.479810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>3.105242</td>\n",
       "      <td>562.939715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>3.212380</td>\n",
       "      <td>545.878970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "      <td>3.010654</td>\n",
       "      <td>537.029102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0</td>\n",
       "      <td>3.520870</td>\n",
       "      <td>534.127500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0</td>\n",
       "      <td>2.907485</td>\n",
       "      <td>532.514699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0</td>\n",
       "      <td>3.014995</td>\n",
       "      <td>472.636769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0</td>\n",
       "      <td>3.400368</td>\n",
       "      <td>693.314506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0</td>\n",
       "      <td>2.848408</td>\n",
       "      <td>390.363563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0</td>\n",
       "      <td>3.443319</td>\n",
       "      <td>629.367403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0</td>\n",
       "      <td>3.242584</td>\n",
       "      <td>648.493892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0</td>\n",
       "      <td>3.122304</td>\n",
       "      <td>714.245338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0</td>\n",
       "      <td>2.621257</td>\n",
       "      <td>622.092646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0</td>\n",
       "      <td>2.782772</td>\n",
       "      <td>585.699533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>614</th>\n",
       "      <td>1</td>\n",
       "      <td>3.865961</td>\n",
       "      <td>758.201732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>615</th>\n",
       "      <td>1</td>\n",
       "      <td>3.302553</td>\n",
       "      <td>650.450324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>616</th>\n",
       "      <td>1</td>\n",
       "      <td>3.045052</td>\n",
       "      <td>711.235436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>617</th>\n",
       "      <td>1</td>\n",
       "      <td>3.922458</td>\n",
       "      <td>593.824578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>618</th>\n",
       "      <td>1</td>\n",
       "      <td>3.037396</td>\n",
       "      <td>665.090554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>619</th>\n",
       "      <td>1</td>\n",
       "      <td>3.307640</td>\n",
       "      <td>735.271839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>620</th>\n",
       "      <td>1</td>\n",
       "      <td>3.412468</td>\n",
       "      <td>724.141783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>621</th>\n",
       "      <td>1</td>\n",
       "      <td>3.738325</td>\n",
       "      <td>632.076120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>622</th>\n",
       "      <td>1</td>\n",
       "      <td>3.313218</td>\n",
       "      <td>665.475028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>623</th>\n",
       "      <td>1</td>\n",
       "      <td>3.492179</td>\n",
       "      <td>590.032019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>624</th>\n",
       "      <td>1</td>\n",
       "      <td>3.410395</td>\n",
       "      <td>551.311711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>625</th>\n",
       "      <td>1</td>\n",
       "      <td>3.532418</td>\n",
       "      <td>677.019051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>626</th>\n",
       "      <td>1</td>\n",
       "      <td>3.486941</td>\n",
       "      <td>667.265346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>627</th>\n",
       "      <td>1</td>\n",
       "      <td>2.929348</td>\n",
       "      <td>749.633740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>628</th>\n",
       "      <td>1</td>\n",
       "      <td>3.764032</td>\n",
       "      <td>611.945766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>629</th>\n",
       "      <td>1</td>\n",
       "      <td>2.944457</td>\n",
       "      <td>624.275140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>630</th>\n",
       "      <td>1</td>\n",
       "      <td>3.583556</td>\n",
       "      <td>657.689211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>631</th>\n",
       "      <td>1</td>\n",
       "      <td>3.567503</td>\n",
       "      <td>570.997409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>632</th>\n",
       "      <td>1</td>\n",
       "      <td>2.948845</td>\n",
       "      <td>683.353304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>633</th>\n",
       "      <td>1</td>\n",
       "      <td>3.088400</td>\n",
       "      <td>644.033320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>634</th>\n",
       "      <td>1</td>\n",
       "      <td>3.785356</td>\n",
       "      <td>633.641188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>635</th>\n",
       "      <td>1</td>\n",
       "      <td>2.947545</td>\n",
       "      <td>712.822653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>636</th>\n",
       "      <td>1</td>\n",
       "      <td>3.868572</td>\n",
       "      <td>658.912044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>637</th>\n",
       "      <td>1</td>\n",
       "      <td>3.440105</td>\n",
       "      <td>702.458099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>638</th>\n",
       "      <td>1</td>\n",
       "      <td>3.257304</td>\n",
       "      <td>689.773376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>639</th>\n",
       "      <td>1</td>\n",
       "      <td>3.381359</td>\n",
       "      <td>720.718438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>640</th>\n",
       "      <td>1</td>\n",
       "      <td>3.083956</td>\n",
       "      <td>556.918021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>641</th>\n",
       "      <td>1</td>\n",
       "      <td>3.114419</td>\n",
       "      <td>734.297679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>642</th>\n",
       "      <td>1</td>\n",
       "      <td>3.549012</td>\n",
       "      <td>604.697503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>643</th>\n",
       "      <td>1</td>\n",
       "      <td>3.532753</td>\n",
       "      <td>588.986175</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>644 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     admit       gpa         gre\n",
       "0        0  3.177277  594.102992\n",
       "1        0  3.412655  631.528607\n",
       "2        0  2.728097  553.714399\n",
       "3        0  3.093559  551.089985\n",
       "4        0  3.141923  537.184894\n",
       "5        0  3.599108  442.763567\n",
       "6        0  3.238972  667.472189\n",
       "7        0  3.420177  561.713905\n",
       "8        0  3.562482  590.340371\n",
       "9        0  3.910495  463.470183\n",
       "10       0  3.264341  636.453166\n",
       "11       0  3.308623  604.405558\n",
       "12       0  3.018922  567.714830\n",
       "13       0  3.151834  503.307298\n",
       "14       0  3.298133  628.178205\n",
       "15       0  2.652201  644.300261\n",
       "16       0  3.115603  533.479810\n",
       "17       0  3.105242  562.939715\n",
       "18       0  3.212380  545.878970\n",
       "19       0  3.010654  537.029102\n",
       "20       0  3.520870  534.127500\n",
       "21       0  2.907485  532.514699\n",
       "22       0  3.014995  472.636769\n",
       "23       0  3.400368  693.314506\n",
       "24       0  2.848408  390.363563\n",
       "25       0  3.443319  629.367403\n",
       "26       0  3.242584  648.493892\n",
       "27       0  3.122304  714.245338\n",
       "28       0  2.621257  622.092646\n",
       "29       0  2.782772  585.699533\n",
       "..     ...       ...         ...\n",
       "614      1  3.865961  758.201732\n",
       "615      1  3.302553  650.450324\n",
       "616      1  3.045052  711.235436\n",
       "617      1  3.922458  593.824578\n",
       "618      1  3.037396  665.090554\n",
       "619      1  3.307640  735.271839\n",
       "620      1  3.412468  724.141783\n",
       "621      1  3.738325  632.076120\n",
       "622      1  3.313218  665.475028\n",
       "623      1  3.492179  590.032019\n",
       "624      1  3.410395  551.311711\n",
       "625      1  3.532418  677.019051\n",
       "626      1  3.486941  667.265346\n",
       "627      1  2.929348  749.633740\n",
       "628      1  3.764032  611.945766\n",
       "629      1  2.944457  624.275140\n",
       "630      1  3.583556  657.689211\n",
       "631      1  3.567503  570.997409\n",
       "632      1  2.948845  683.353304\n",
       "633      1  3.088400  644.033320\n",
       "634      1  3.785356  633.641188\n",
       "635      1  2.947545  712.822653\n",
       "636      1  3.868572  658.912044\n",
       "637      1  3.440105  702.458099\n",
       "638      1  3.257304  689.773376\n",
       "639      1  3.381359  720.718438\n",
       "640      1  3.083956  556.918021\n",
       "641      1  3.114419  734.297679\n",
       "642      1  3.549012  604.697503\n",
       "643      1  3.532753  588.986175\n",
       "\n",
       "[644 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "admissions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2674523ec18>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Logistic Function\n",
    "def logistic(x):\n",
    "    # np.exp(x) raises x to the exponential power, ie e^x. e ~= 2.71828\n",
    "    return np.exp(x)  / (1 + np.exp(x)) \n",
    "    \n",
    "# Generate 50 real values, evenly spaced, between -6 and 6.\n",
    "x = np.linspace(-6,6,50, dtype=float)\n",
    "\n",
    "# Transform each number in t using the logistic function.\n",
    "y = logistic(x)\n",
    "\n",
    "# Plot the resulting data.\n",
    "plt.plot(x, y)\n",
    "plt.ylabel(\"Probability\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a logistic regression model\n",
    "\n",
    "- Import the LogisticRegression class and instantiate a model named logistic_model.\n",
    "- Use the LogisticRegression method fit to fit the model to the data. We're only interested in constructing a model that uses gpa values to predict admit values.\n",
    "- View the documentation for the LogisticRegression class if you get stuck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logistic_model = LogisticRegression()\n",
    "logistic_model.fit(admissions[[\"gpa\"]], admissions[\"admit\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting probabilities\n",
    "\n",
    "- Use the LogisticRegression method predict_proba to return the predicted probabilities for the data in the gpa column. Assign the returned probabilities to pred_probs.\n",
    "\n",
    "- Create and display a scatter plot using the Matplotlib scatter function where:\n",
    "\n",
    "    - the x-axis is the values in the gpa column,\n",
    "    - the y-axis is the probability of being classified as label 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x267471a3da0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2674715ac18>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pred_probs= logistic_model.predict_proba(admissions[[\"gpa\"]])\n",
    "# Probability that the row belongs to label `0`.\n",
    "pred_probs[:,0]\n",
    "# Probabililty that the row belongs to label `1`.\n",
    "pred_probs[:,1]\n",
    "plt.scatter(admissions[\"gpa\"],pred_probs[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.63387918 0.36612082]\n",
      " [0.57129471 0.42870529]\n",
      " [0.74047131 0.25952869]\n",
      " ...\n",
      " [0.64994531 0.35005469]\n",
      " [0.53382227 0.46617773]\n",
      " [0.53831911 0.46168089]]\n"
     ]
    }
   ],
   "source": [
    "print(pred_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict labels\n",
    "\n",
    "- Use the LogisticRegression method predict to return the predicted for each label in the training set.\n",
    "\n",
    " - The parameter for the predict method matches that of the predict_proba method:\n",
    "        - X: rows of data to use for prediction.\n",
    "        - Assign the result to fitted_labels.\n",
    "- Use the print function to display the first 10 values in fitted_labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x26747209f60>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2674717e9b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "logistic_model = LogisticRegression()\n",
    "logistic_model.fit(admissions[[\"gpa\"]], admissions[\"admit\"])\n",
    "fitted_labels= logistic_model.predict(admissions[[\"gpa\"]])\n",
    "# Probability that the row belongs to label `0`.\n",
    "plt.scatter(admissions[\"gpa\"], fitted_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 644 entries, 0 to 643\n",
      "Data columns (total 3 columns):\n",
      "admit    644 non-null int64\n",
      "gpa      644 non-null float64\n",
      "gre      644 non-null float64\n",
      "dtypes: float64(2), int64(1)\n",
      "memory usage: 15.2 KB\n"
     ]
    }
   ],
   "source": [
    "admissions.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to evaluating binary classifiers\n",
    "\n",
    "- Use the LogisticRegression method predict to return the label for each observation in the dataset, admissions. Assign the returned list to labels.\n",
    "- Add a new column to the admissions Dataframe named predicted_label that contains the values from labels.\n",
    "- Use the Series method value_counts and the print function to display the distribution of the values in the predicted_label column.\n",
    "- Use the Dataframe method head and the print function to display the first 5 rows in admissions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "admissions = pd.read_csv(\"admissions.csv\")\n",
    "model = LogisticRegression()\n",
    "model.fit(admissions[[\"gpa\"]], admissions[\"admit\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    598\n",
      "1     46\n",
      "Name: predicted_label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "labels = model.predict(admissions[[\"gpa\"]])\n",
    "admissions[\"predicted_label\"] = labels\n",
    "print(admissions[\"predicted_label\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy\n",
    "\n",
    "- Rename the admit column from the admissions Dataframe to actual_label so it's more clear which column contains the predicted labels (predicted_label) and which column contains the actual labels (actual_label).\n",
    "- Compare the predicted_label column with the actual_label column.\n",
    "Use a double equals sign (==) to compare the 2 Series objects and assign the resulting Series object to matches.\n",
    "    - Use conditional filtering to filter admissions to just the rows where matches is True. Assign the resulting Dataframe to correct_predictions.\n",
    "- Display the first 5 rows in correct_predictions to make sure the values in the predicted_label and actual_label columns are equal.\n",
    "- Calculate the accuracy and assign the resulting float value to accuracy.\n",
    "    - Display accuracy using the print function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'actual_label'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2524\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2525\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2526\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'actual_label'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-53e1221afe96>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#admissions[\"actual_label\"] = admissions[\"admit\"]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmatches\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0madmissions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"predicted_label\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0madmissions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"actual_label\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mcorrect_predictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0madmissions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmatches\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorrect_predictions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcorrect_predictions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0madmissions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2137\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2138\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2139\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2140\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2141\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_getitem_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_getitem_column\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2144\u001b[0m         \u001b[1;31m# get column\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2145\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2146\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_item_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2147\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2148\u001b[0m         \u001b[1;31m# duplicate columns & possible reduce dimensionality\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_get_item_cache\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m   1840\u001b[0m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1841\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1842\u001b[1;33m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1843\u001b[0m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_box_item_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1844\u001b[0m             \u001b[0mcache\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, item, fastpath)\u001b[0m\n\u001b[0;32m   3841\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3842\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3843\u001b[1;33m                 \u001b[0mloc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3844\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3845\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0misna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2525\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2526\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2527\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2528\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2529\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'actual_label'"
     ]
    }
   ],
   "source": [
    "#admissions[\"actual_label\"] = admissions[\"admit\"]\n",
    "matches = admissions[\"predicted_label\"] == admissions[\"actual_label\"]\n",
    "correct_predictions = admissions[matches]\n",
    "print(correct_predictions.head())\n",
    "accuracy = correct_predictions.shape[0] / admissions.shape[0]\n",
    "print(accuracy)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary classification outcome\n",
    "\n",
    "- Extract all of the rows where predicted_label and actual_label both equal 1. Then, calculate the number of true positives and assign to true_positives.\n",
    "\n",
    "- Extract all of the rows where predicted_label and actual_label both equal 0. Then, calculate the number of true negatives and assign to true_negatives.\n",
    "\n",
    "- Display both true_positives and true_negatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_positive_filter = (admissions[\"predicted_label\"] == 1) & (admissions[\"actual_label\"] == 1)\n",
    "true_positives = len(admissions[true_positive_filter])\n",
    "\n",
    "true_negative_filter = (admissions[\"predicted_label\"] == 0) & (admissions[\"actual_label\"] == 0)\n",
    "true_negatives = len(admissions[true_negative_filter])\n",
    "\n",
    "print(true_positives)\n",
    "print(true_negatives)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sensitivity\n",
    "\n",
    "- Calculate the number of false negatives (where the model predicted rejected but the student was actually admitted) and assign to false_negatives.\n",
    "- Calculate the sensitivity and assign the computed value to sensitivity.\n",
    "- Display sensitivity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_negative_filter= (admissions[\"predicted_label\"] == 0) & (admissions[\"actual_label\"] == 1)\n",
    "false_negatives=len(false_negative_filter)\n",
    "sensitivity=true_positives / (true_positives + false_negatives)\n",
    "print(sensitivity)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specificity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_positive_filter = (admissions[\"predicted_label\"] == 1) & (admissions[\"actual_label\"] == 1)\n",
    "true_positives = len(admissions[true_positive_filter])\n",
    "false_negative_filter = (admissions[\"predicted_label\"] == 0) & (admissions[\"actual_label\"] == 1)\n",
    "false_negatives = len(admissions[false_negative_filter])\n",
    "true_negative_filter = (admissions[\"predicted_label\"] == 0) & (admissions[\"actual_label\"] == 0)\n",
    "true_negatives = len(admissions[true_negative_filter])\n",
    "false_positive_filter = (admissions[\"predicted_label\"] == 1) & (admissions[\"actual_label\"] == 0)\n",
    "false_positives = len(admissions[false_positive_filter])\n",
    "specificity = (true_negatives) / (false_positives + true_negatives)\n",
    "print(specificity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiclass classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction of the data\n",
    "\n",
    "- Import the Pandas library and read auto.csv into a Dataframe named cars.\n",
    "\n",
    "- Use the Series.unique() method to assign the unique elements in the column origin to unique_regions. Then use the print function to display unique_regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "cars = pd.read_csv(\"auto.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 3, 2], dtype=int64)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_regions=cars[\"origin\"].unique()\n",
    "unique_regions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dummy variables\n",
    "\n",
    "- Use the pandas.get_dummies() function to create dummy values from the year column.\n",
    "\n",
    "    - Use the prefix attribute to prepend year to each of the resulting column names.\n",
    "    - Assign the resulting Dataframe to dummy_years.\n",
    "- Use the pandas.concat() function to concatenate the columns from dummy_years to cars.\n",
    "- Use the DataFrame.drop() method to drop the year and cylinders columns from cars.\n",
    "- Display the first 5 rows of the new cars Dataframe to confirm.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mpg</th>\n",
       "      <th>displacement</th>\n",
       "      <th>horsepower</th>\n",
       "      <th>weight</th>\n",
       "      <th>acceleration</th>\n",
       "      <th>origin</th>\n",
       "      <th>cyl_3</th>\n",
       "      <th>cyl_4</th>\n",
       "      <th>cyl_5</th>\n",
       "      <th>cyl_6</th>\n",
       "      <th>...</th>\n",
       "      <th>year_73</th>\n",
       "      <th>year_74</th>\n",
       "      <th>year_75</th>\n",
       "      <th>year_76</th>\n",
       "      <th>year_77</th>\n",
       "      <th>year_78</th>\n",
       "      <th>year_79</th>\n",
       "      <th>year_80</th>\n",
       "      <th>year_81</th>\n",
       "      <th>year_82</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>3504.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15.0</td>\n",
       "      <td>350.0</td>\n",
       "      <td>165.0</td>\n",
       "      <td>3693.0</td>\n",
       "      <td>11.5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18.0</td>\n",
       "      <td>318.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>3436.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16.0</td>\n",
       "      <td>304.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>3433.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17.0</td>\n",
       "      <td>302.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>3449.0</td>\n",
       "      <td>10.5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    mpg  displacement  horsepower  weight  acceleration  origin  cyl_3  cyl_4  \\\n",
       "0  18.0         307.0       130.0  3504.0          12.0       1      0      0   \n",
       "1  15.0         350.0       165.0  3693.0          11.5       1      0      0   \n",
       "2  18.0         318.0       150.0  3436.0          11.0       1      0      0   \n",
       "3  16.0         304.0       150.0  3433.0          12.0       1      0      0   \n",
       "4  17.0         302.0       140.0  3449.0          10.5       1      0      0   \n",
       "\n",
       "   cyl_5  cyl_6   ...     year_73  year_74  year_75  year_76  year_77  \\\n",
       "0      0      0   ...           0        0        0        0        0   \n",
       "1      0      0   ...           0        0        0        0        0   \n",
       "2      0      0   ...           0        0        0        0        0   \n",
       "3      0      0   ...           0        0        0        0        0   \n",
       "4      0      0   ...           0        0        0        0        0   \n",
       "\n",
       "   year_78  year_79  year_80  year_81  year_82  \n",
       "0        0        0        0        0        0  \n",
       "1        0        0        0        0        0  \n",
       "2        0        0        0        0        0  \n",
       "3        0        0        0        0        0  \n",
       "4        0        0        0        0        0  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_cylinders = pd.get_dummies(cars[\"cylinders\"], prefix=\"cyl\")\n",
    "cars = pd.concat([cars, dummy_cylinders], axis=1)\n",
    "dummy_years = pd.get_dummies(cars[\"year\"], prefix=\"year\")\n",
    "cars = pd.concat([cars, dummy_years], axis=1)\n",
    "cars=cars.drop(columns=[\"cylinders\",\"year\"],axis=1)\n",
    "cars.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiclass classification\n",
    "\n",
    "- Split the shuffled_cars Dataframe into 2 Dataframes: train and test.\n",
    "- Assign the first 70% of the shuffled_cars to train.\n",
    "- Assign the last 30% of the shuffled_cars to test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_rows = np.random.permutation(cars.index)\n",
    "shuffled_cars = cars.iloc[shuffled_rows]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_rows = np.random.permutation(cars.index)\n",
    "shuffled_cars = cars.iloc[shuffled_rows]\n",
    "slip=int(cars.shape[0] * 0.7)\n",
    "train=shuffled_cars.iloc[0:slip]\n",
    "test=shuffled_cars.iloc[slip:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "shuffled_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RangeIndex(start=0, stop=392, step=1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cars.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a multiclass logistic regression model\n",
    "\n",
    "For each value in unique_origins, train a logistic regression model with the following parameters:\n",
    "\n",
    "- X: Dataframe containing just the cylinder & year binary columns.\n",
    "- y: list (or Series) of Boolean values:\n",
    "    - True if observation's value for origin matches the current iterator variable.\n",
    "    - False if observation's value for origin doesn't match the current iterator variable.\n",
    "\n",
    "Add each model to the models dictionary with the following structure:\n",
    "\n",
    "    - key: origin value (1, 2, or 3),\n",
    "    - value: relevant LogistcRegression model instance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "unique_origins = cars[\"origin\"].unique()\n",
    "unique_origins.sort()\n",
    "\n",
    "models = {}\n",
    "features = [c for c in train.columns if c.startswith(\"cyl\") or c.startswith(\"year\")]\n",
    "        \n",
    "\n",
    "for origin in unique_origins:\n",
    "    model = LogisticRegression()\n",
    "    \n",
    "    X_train = train[features]\n",
    "    y_train = train[\"origin\"] == origin\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    models[origin] = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "           penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "           verbose=0, warm_start=False),\n",
       " 2: LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "           penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "           verbose=0, warm_start=False),\n",
       " 3: LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "           penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "           verbose=0, warm_start=False)}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the model\n",
    "\n",
    "For each origin value from unique_origins:\n",
    "\n",
    "Use the LogisticRegression predict_proba function to return the 3 lists of predicted probabilities for the test set and add to the testing_probs Dataframe.\n",
    "Here's how the final Dataframe should look like (without all zeroes of course!):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_probs = pd.DataFrame(columns=unique_origins)  \n",
    "\n",
    "for origin in unique_origins:\n",
    "    # Select testing features.\n",
    "    X_test = test[features]   \n",
    "    # Compute probability of observation being in the origin.\n",
    "    testing_probs[origin] = models[origin].predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chose the origin\n",
    "\n",
    "- Classify each observation in the test set using the testing_probs Dataframe.\n",
    "- Assign the predicted origins to predicted_origins and use the print function to display it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_origins = testing_probs.idxmax(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "columns = [\"mpg\", \"cylinders\", \"displacement\", \"horsepower\", \"weight\", \"acceleration\", \"model year\", \"origin\", \"car name\"]\n",
    "cars = pd.read_table(\"auto-mpg.data\", delim_whitespace=True, names=columns)\n",
    "filtered_cars = cars[cars['horsepower'] != '?'].copy()\n",
    "filtered_cars['horsepower'] = filtered_cars['horsepower'].astype('float')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias variance tradeoff\n",
    "\n",
    "- Create a function named train_and_test that:\n",
    "\n",
    "    - Takes in a list of column names in filtered_cars as the sole parameter (cols),\n",
    "    - Trains a linear regression model using:\n",
    "        - The columns in cols as the features,\n",
    "        - The mpg column as the target variable.\n",
    "    - Uses the trained model to make predictions using the same input it was trained on,\n",
    "    - Computes the variance of the predicted values and the mean squared error between the predicted values and the actual label (mpg column).\n",
    "    - Returns the mean squared error value followed by the variance (e.g. return(mse, variance)).\n",
    "- Use the train_and_test function to train a model using only the cylinders column. Assign the resulting mean squared error value and variance to cyl_mse and cyl_var.\n",
    "\n",
    "- Use the train_and_test function to train a model using only the weight column. Assign the resulting mean squared error value and variance to weight_mse and weight_var."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "def train_and_test(cols):\n",
    "    features=cols\n",
    "    target=filtered_cars[\"mpg\"]\n",
    "    model=LinearRegression()\n",
    "    model.fit(features,target)\n",
    "    y_predict=model.predict(features)\n",
    "    variance=np.var(y_predict)\n",
    "    mse=mean_squared_error(y_predict,target)\n",
    "    return(mse, variance)\n",
    "\n",
    "cyl_mse,cyl_var=train_and_test(filtered_cars[[\"cylinders\"]])\n",
    "weight_mse,weight_var=train_and_test(filtered_cars[[\"weight\"]])\n",
    "  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42.08612184489641"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multivariance model\n",
    "\n",
    "Use the train_and_test function to train linear regression models using the following columns as the features:\n",
    "\n",
    "- columns: cylinders, displacement.\n",
    "    - MSE: two_mse, variance: two_var.\n",
    "- columns: cylinders, displacement, horsepower.\n",
    "    - MSE: three_mse, variance: three_var.\n",
    "- columns: cylinders, displacement, horsepower, weight.\n",
    "    - MSE: four_mse, variance: four_var.\n",
    "- columns: cylinders, displacement, horsepower, weight, acceleration.\n",
    "    - MSE: five_mse, variance: five_var.\n",
    "- columns: cylinders, displacement, horsepower, weight, acceleration, model year\n",
    "    - MSE: six_mse, variance: six_var.\n",
    "- columns: cylinders, displacement, horsepower, weight, acceleration, model year, origin\n",
    "    - MSE: seven_mse, variance: seven_var.\n",
    "Use print statements or the variable inspector to display each value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_mse,two_var=train_and_test(filtered_cars[[\"cylinders\", \"displacement\"]])\n",
    "three_mse,three_var=train_and_test(filtered_cars[[\"cylinders\", \"displacement\",\"horsepower\"]])\n",
    "four_mse,four_var=train_and_test(filtered_cars[[\"cylinders\", \"displacement\",\"horsepower\",\"weight\"]])\n",
    "five_mse,five_var=train_and_test(filtered_cars[[\"cylinders\", \"displacement\",\"horsepower\",\"weight\",\"acceleration\"]])\n",
    "six_mse,six_var=train_and_test(filtered_cars[[\"cylinders\", \"displacement\",\"horsepower\",\"weight\",\"acceleration\",\"model year\"]])\n",
    "seven_mse,seven_var=train_and_test(filtered_cars[[\"cylinders\", \"displacement\",\"horsepower\",\"weight\",\"acceleration\",\"model year\",\"origin\"]])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista= [two_mse,two_var,three_mse,three_var,four_mse,four_var,five_mse,five_var,six_mse,six_var,seven_mse,seven_var]\n",
    "lista1 = []\n",
    "for i in lista:\n",
    "    lista1.append(i)\n",
    "test=lista1\n",
    "test = pd.DataFrame({'col':lista1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>21.282057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>39.480681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20.252955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>40.509784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17.763861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>42.998878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>17.761396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>43.001342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>11.590171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>49.172567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10.847481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>49.915257</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          col\n",
       "0   21.282057\n",
       "1   39.480681\n",
       "2   20.252955\n",
       "3   40.509784\n",
       "4   17.763861\n",
       "5   42.998878\n",
       "6   17.761396\n",
       "7   43.001342\n",
       "8   11.590171\n",
       "9   49.172567\n",
       "10  10.847481\n",
       "11  49.915257"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation\n",
    "\n",
    "Create a function named train_and_cross_val that:\n",
    "\n",
    "- takes in a single parameter (list of column names),\n",
    "- trains a linear regression model using the features specified in the parameter,\n",
    "- uses the KFold class to perform 10-fold validation using a random seed of 3 (we use this seed to answer check your code),\n",
    "- calculates the mean squared error across all folds and the mean variance across all folds.\n",
    "- returns the mean squared error value then the variance using a multiple return statement (e.g. return(avg_mse, avg_var)).\n",
    "\n",
    "Use the train_and_cross_val function to train linear regression models using the following columns as the features:\n",
    "\n",
    "- the cylinders and displacement columns. Assign the resulting mean squared error value to two_mse and the resulting variance value to two_var.\n",
    "- the cylinders, displacement, and horsepower columns. Assign the resulting mean squared error value to three_mse and the resulting variance value to three_var.\n",
    "- the cylinders, displacement, horsepower, and weight columns. Assign the resulting mean squared error value to four_mse and the resulting variance value to four_var.\n",
    "- the cylinders, displacement, horsepower, weight, acceleration columns. Assign the resulting mean squared error value to five_mse and the resulting variance value to five_var.\n",
    "- the cylinders, displacement, horsepower, weight, acceleration, and model year columns. Assign the resulting mean squared error value to six_mse and the resulting variance value to six_var.\n",
    "- the cylinders, displacement, horsepower, weight, acceleration, model year, and origin columns. Assign the resulting mean squared error value to seven_mse and the resulting variance value to seven_var.\n",
    "\n",
    "Use the variable display to inspect each value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "def train_and_cross_val(cols):\n",
    "    features=cols\n",
    "    target=filtered_cars[\"mpg\"]\n",
    "    model=LinearRegression()\n",
    "    y_test=model.fit(features,target)\n",
    "    kfold=KFold(n_splits=10, random_state=3, shuffle=False)\n",
    "    kfold.get_n_splits(features) \n",
    "    y_predict=model.predict(features)\n",
    "    variance=np.var(y_predict)\n",
    "    mse=mean_squared_error(y_predict,target)\n",
    "    avg_var=np.mean(variance)\n",
    "    avg_mse=mse\n",
    "    return(avg_mse, avg_var)\n",
    "\n",
    "two_mse,two_var=train_and_cross_val(filtered_cars[[\"cylinders\", \"displacement\"]])\n",
    "three_mse,three_var=train_and_cross_val(filtered_cars[[\"cylinders\", \"displacement\",\"horsepower\"]])\n",
    "four_mse,four_var=train_and_cross_val(filtered_cars[[\"cylinders\", \"displacement\",\"horsepower\",\"weight\"]])\n",
    "five_mse,five_var=train_and_cross_val(filtered_cars[[\"cylinders\", \"displacement\",\"horsepower\",\"weight\",\"acceleration\"]])\n",
    "six_mse,six_var=train_and_cross_val(filtered_cars[[\"cylinders\", \"displacement\",\"horsepower\",\"weight\",\"acceleration\",\"model year\"]])\n",
    "seven_mse,seven_var=train_and_cross_val(filtered_cars[[\"cylinders\", \"displacement\",\"horsepower\",\"weight\",\"acceleration\",\"model year\",\"origin\"]])\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "def train_and_cross_val(cols):\n",
    "    features = filtered_cars[cols]\n",
    "    target = filtered_cars[\"mpg\"]\n",
    "    \n",
    "    variance_values = []\n",
    "    mse_values = []\n",
    "    \n",
    "    # KFold instance.\n",
    "    kf = KFold(n_splits=10, shuffle=True, random_state=3)\n",
    "    \n",
    "    # Iterate through over each fold.\n",
    "    for train_index, test_index in kf.split(features):\n",
    "        # Training and test sets.\n",
    "        X_train, X_test = features.iloc[train_index], features.iloc[test_index]\n",
    "        y_train, y_test = target.iloc[train_index],target.iloc[test_index]\n",
    "        \n",
    "        # Fit the model and make predictions.\n",
    "        lr = LinearRegression()\n",
    "        lr.fit(X_train, y_train)\n",
    "        predictions = lr.predict(X_test)\n",
    "        \n",
    "        # Calculate mse and variance values for this fold.\n",
    "        mse = mean_squared_error(y_test, predictions)\n",
    "        var = np.var(predictions)\n",
    "\n",
    "        # Append to arrays to do calculate overall average mse and variance values.\n",
    "        variance_values.append(var)\n",
    "        mse_values.append(mse)\n",
    "   \n",
    "    # Compute average mse and variance values.\n",
    "    avg_mse = np.mean(mse_values)\n",
    "    avg_var = np.mean(variance_values)\n",
    "    return(avg_mse, avg_var)\n",
    "\n",
    "two_mse, two_var = train_and_cross_val([\"cylinders\", \"displacement\"])\n",
    "three_mse, three_var = train_and_cross_val([\"cylinders\", \"displacement\", \"horsepower\"])\n",
    "four_mse, four_var = train_and_cross_val([\"cylinders\", \"displacement\", \"horsepower\", \"weight\"])\n",
    "five_mse, five_var = train_and_cross_val([\"cylinders\", \"displacement\", \"horsepower\", \"weight\", \"acceleration\"])\n",
    "six_mse, six_var = train_and_cross_val([\"cylinders\", \"displacement\", \"horsepower\", \"weight\", \"acceleration\", \"model year\"])\n",
    "seven_mse, seven_var = train_and_cross_val([\"cylinders\", \"displacement\", \"horsepower\", \"weight\", \"acceleration\",\"model year\", \"origin\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49.90431373098729"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seven_var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Plotting cross-validation error vs. cross-validation variance\n",
    "\n",
    "- On the same Axes instance:\n",
    "\n",
    "    - Generate a scatter plot with the model's number of features on the x-axis and the model's overall, cross-validation mean squared error on the y-axis. Use red for the scatter dot color.\n",
    "    - Generate a scatter plot with the model's number of features on the x-axis and the model's overall, cross-validation variance on the y-axis. Use blue for the scatter dot color.\n",
    "    \n",
    "- Use plt.show() to display the scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2aee46d3eb8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# We've hidden the `train_and_cross_val` function to save space but you can still call the function!\n",
    "import matplotlib.pyplot as plt\n",
    "        \n",
    "two_mse, two_var = train_and_cross_val([\"cylinders\", \"displacement\"])\n",
    "three_mse, three_var = train_and_cross_val([\"cylinders\", \"displacement\", \"horsepower\"])\n",
    "four_mse, four_var = train_and_cross_val([\"cylinders\", \"displacement\", \"horsepower\", \"weight\"])\n",
    "five_mse, five_var = train_and_cross_val([\"cylinders\", \"displacement\", \"horsepower\", \"weight\", \"acceleration\"])\n",
    "six_mse, six_var = train_and_cross_val([\"cylinders\", \"displacement\", \"horsepower\", \"weight\", \"acceleration\", \"model year\"])\n",
    "seven_mse, seven_var = train_and_cross_val([\"cylinders\", \"displacement\", \"horsepower\", \"weight\", \"acceleration\",\"model year\", \"origin\"])\n",
    "\n",
    "plt.scatter([2,3,4,5,6,7], [two_mse, three_mse, four_mse, five_mse, six_mse, seven_mse], c='red')\n",
    "plt.scatter([2,3,4,5,6,7], [two_var, three_var, four_var, five_var, six_var, seven_var], c='blue')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cyl_3',\n",
       " 'cyl_4',\n",
       " 'cyl_5',\n",
       " 'cyl_6',\n",
       " 'cyl_8',\n",
       " 'year_70',\n",
       " 'year_71',\n",
       " 'year_72',\n",
       " 'year_73',\n",
       " 'year_74',\n",
       " 'year_75',\n",
       " 'year_76',\n",
       " 'year_77',\n",
       " 'year_78',\n",
       " 'year_79',\n",
       " 'year_80',\n",
       " 'year_81',\n",
       " 'year_82']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
